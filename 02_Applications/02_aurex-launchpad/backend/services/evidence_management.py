# ================================================================================
# AUREX LAUNCHPADâ„¢ EVIDENCE MANAGEMENT SERVICE
# Sub-Application #13: Advanced Evidence Processing and Validation
# Module ID: LAU-MAT-013 - Evidence Management Service
# Created: August 7, 2025
# ================================================================================

from typing import Dict, List, Optional, Tuple, Any, BinaryIO
from dataclasses import dataclass
from enum import Enum
import os
import shutil
import hashlib
import mimetypes
import magic
from datetime import datetime, timedelta
import uuid
import json
import asyncio
from PIL import Image, ImageEnhance
import pytesseract
import PyPDF2
import pandas as pd
import zipfile
import tempfile
import logging
from pathlib import Path
import aiofiles
from fastapi import UploadFile, HTTPException
import boto3
from botocore.exceptions import ClientError

# Import models
from models.carbon_maturity_models import (
    AssessmentEvidence, EvidenceType, AssessmentResponse, MaturityAssessment
)

# Configure logging
logger = logging.getLogger(__name__)

# ================================================================================
# EVIDENCE VALIDATION AND PROCESSING CONFIGURATION
# ================================================================================

@dataclass
class FileValidationRules:
    """File validation rules and constraints"""
    max_file_size: int = 50 * 1024 * 1024  # 50MB
    min_file_size: int = 1024  # 1KB
    allowed_mime_types: List[str] = None
    allowed_extensions: List[str] = None
    virus_scan_required: bool = True
    content_validation: bool = True
    
    def __post_init__(self):
        if self.allowed_mime_types is None:
            self.allowed_mime_types = [
                'application/pdf',
                'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                'application/vnd.ms-excel',
                'text/csv',
                'text/plain',
                'image/jpeg',
                'image/png',
                'image/tiff',
                'image/bmp',
                'application/msword',
                'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                'application/zip',
                'application/json'
            ]
        
        if self.allowed_extensions is None:
            self.allowed_extensions = [
                '.pdf', '.xlsx', '.xls', '.csv', '.txt', '.doc', '.docx',
                '.jpg', '.jpeg', '.png', '.tiff', '.tif', '.bmp',
                '.zip', '.json'
            ]

@dataclass
class ProcessingResult:
    """Result of evidence processing"""
    success: bool
    evidence_id: str
    file_path: str
    extracted_text: Optional[str] = None
    metadata: Dict[str, Any] = None
    processing_time: float = 0.0
    errors: List[str] = None
    warnings: List[str] = None
    
    def __post_init__(self):
        if self.errors is None:
            self.errors = []
        if self.warnings is None:
            self.warnings = []

class ProcessingStatus(Enum):
    """Evidence processing status"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    QUARANTINED = "quarantined"

# ================================================================================
# EVIDENCE MANAGEMENT SERVICE
# ================================================================================

class EvidenceManagementService:
    """
    Comprehensive evidence management service with advanced processing capabilities
    Handles file upload, validation, processing, storage, and retrieval
    """
    
    def __init__(self, storage_config: Dict[str, Any] = None):
        self.storage_config = storage_config or self._get_default_storage_config()
        self.validation_rules = FileValidationRules()
        self.processing_queue = asyncio.Queue()
        self._setup_storage_directories()
        self._setup_aws_client()
    
    def _get_default_storage_config(self) -> Dict[str, Any]:
        """Get default storage configuration"""
        return {
            "base_path": "uploads/evidence",
            "use_cloud_storage": os.getenv("USE_CLOUD_STORAGE", "false").lower() == "true",
            "aws_bucket": os.getenv("AWS_S3_BUCKET", "aurex-evidence-storage"),
            "aws_region": os.getenv("AWS_REGION", "us-east-1"),
            "retention_days": int(os.getenv("EVIDENCE_RETENTION_DAYS", "1825")),  # 5 years
            "enable_encryption": True,
            "enable_virus_scan": True,
            "enable_content_analysis": True
        }
    
    def _setup_storage_directories(self):
        """Setup local storage directory structure"""
        base_path = Path(self.storage_config["base_path"])
        
        directories = [
            "uploads",
            "processed",
            "quarantine",
            "temporary",
            "thumbnails",
            "backups"
        ]
        
        for directory in directories:
            (base_path / directory).mkdir(parents=True, exist_ok=True)
        
        logger.info("Evidence storage directories initialized")
    
    def _setup_aws_client(self):
        """Setup AWS S3 client for cloud storage"""
        if self.storage_config.get("use_cloud_storage"):
            try:
                self.s3_client = boto3.client(
                    's3',
                    region_name=self.storage_config["aws_region"]
                )
                logger.info("AWS S3 client initialized")
            except Exception as e:
                logger.error(f"Failed to initialize AWS S3 client: {e}")
                self.s3_client = None
        else:
            self.s3_client = None
    
    async def upload_evidence(
        self,
        file: UploadFile,
        assessment_id: str,
        response_id: Optional[str],
        evidence_type: EvidenceType,
        title: str,
        description: Optional[str],
        uploaded_by: str,
        db_session = None
    ) -> ProcessingResult:
        """
        Upload and process evidence file with comprehensive validation
        
        Args:
            file: Uploaded file
            assessment_id: Assessment UUID
            response_id: Optional response UUID
            evidence_type: Type of evidence
            title: Evidence title
            description: Evidence description
            uploaded_by: User ID who uploaded the file
            db_session: Database session
            
        Returns:
            ProcessingResult with upload and processing details
        """
        
        start_time = datetime.utcnow()
        processing_result = ProcessingResult(
            success=False,
            evidence_id="",
            file_path=""
        )
        
        try:
            # Step 1: Pre-upload validation
            validation_result = await self._validate_file(file)
            if not validation_result["valid"]:
                processing_result.errors.extend(validation_result["errors"])
                return processing_result
            
            # Step 2: Generate unique identifiers
            evidence_id = str(uuid.uuid4())
            unique_filename = self._generate_unique_filename(file.filename, evidence_id)
            
            # Step 3: Create temporary file for processing
            temp_file_path = await self._save_temporary_file(file, unique_filename)
            
            # Step 4: Advanced file validation
            advanced_validation = await self._advanced_file_validation(temp_file_path)
            if not advanced_validation["valid"]:
                processing_result.errors.extend(advanced_validation["errors"])
                processing_result.warnings.extend(advanced_validation["warnings"])
                
                # Move to quarantine if serious issues
                if advanced_validation["quarantine"]:
                    await self._quarantine_file(temp_file_path, evidence_id, "Failed validation")
                    processing_result.success = False
                    return processing_result
            
            # Step 5: Virus scan (if enabled)
            if self.storage_config.get("enable_virus_scan", True):
                virus_scan_result = await self._virus_scan(temp_file_path)
                if not virus_scan_result["clean"]:
                    await self._quarantine_file(temp_file_path, evidence_id, "Virus detected")
                    processing_result.errors.append("File failed virus scan")
                    return processing_result
            
            # Step 6: Extract file metadata
            file_metadata = await self._extract_file_metadata(temp_file_path, file)
            
            # Step 7: Process file content (OCR, text extraction)
            content_processing_result = await self._process_file_content(
                temp_file_path, evidence_type, file_metadata
            )
            
            # Step 8: Store file (local or cloud)
            final_file_path = await self._store_file(
                temp_file_path, assessment_id, evidence_id, unique_filename
            )
            
            # Step 9: Generate thumbnail (for images)
            thumbnail_path = None
            if file_metadata["mime_type"].startswith("image/"):
                thumbnail_path = await self._generate_thumbnail(final_file_path, evidence_id)
            
            # Step 10: Create database record
            evidence_record = await self._create_evidence_record(
                evidence_id=evidence_id,
                assessment_id=assessment_id,
                response_id=response_id,
                file_metadata=file_metadata,
                file_path=final_file_path,
                evidence_type=evidence_type,
                title=title,
                description=description,
                uploaded_by=uploaded_by,
                extracted_text=content_processing_result.get("extracted_text"),
                processing_metadata=content_processing_result.get("metadata", {}),
                db_session=db_session
            )
            
            # Step 11: Cleanup temporary file
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)
            
            # Step 12: Prepare success response
            processing_time = (datetime.utcnow() - start_time).total_seconds()
            
            processing_result.success = True
            processing_result.evidence_id = evidence_id
            processing_result.file_path = final_file_path
            processing_result.extracted_text = content_processing_result.get("extracted_text")
            processing_result.metadata = file_metadata
            processing_result.processing_time = processing_time
            
            # Add any processing warnings
            if content_processing_result.get("warnings"):
                processing_result.warnings.extend(content_processing_result["warnings"])
            
            logger.info(f"Successfully processed evidence {evidence_id} in {processing_time:.2f}s")
            
            return processing_result
            
        except Exception as e:
            logger.error(f"Failed to process evidence: {str(e)}")
            processing_result.errors.append(f"Processing failed: {str(e)}")
            
            # Cleanup on failure
            try:
                if 'temp_file_path' in locals() and os.path.exists(temp_file_path):
                    os.remove(temp_file_path)
            except:
                pass
            
            return processing_result
    
    async def _validate_file(self, file: UploadFile) -> Dict[str, Any]:
        """Pre-upload file validation"""
        
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": []
        }
        
        # Check file size
        file_content = await file.read()
        file_size = len(file_content)
        await file.seek(0)  # Reset file pointer
        
        if file_size > self.validation_rules.max_file_size:
            validation_result["valid"] = False
            validation_result["errors"].append(
                f"File size {file_size} exceeds maximum allowed size {self.validation_rules.max_file_size}"
            )
        
        if file_size < self.validation_rules.min_file_size:
            validation_result["valid"] = False
            validation_result["errors"].append(
                f"File size {file_size} is below minimum required size {self.validation_rules.min_file_size}"
            )
        
        # Check file extension
        file_extension = os.path.splitext(file.filename)[1].lower()
        if file_extension not in self.validation_rules.allowed_extensions:
            validation_result["valid"] = False
            validation_result["errors"].append(f"File extension {file_extension} not allowed")
        
        # Basic filename validation
        if not file.filename or len(file.filename) > 255:
            validation_result["valid"] = False
            validation_result["errors"].append("Invalid filename")
        
        # Check for suspicious filenames
        suspicious_patterns = ['..', '/', '\\', '<script', '<?php']
        if any(pattern in file.filename.lower() for pattern in suspicious_patterns):
            validation_result["valid"] = False
            validation_result["errors"].append("Suspicious filename detected")
        
        return validation_result
    
    async def _save_temporary_file(self, file: UploadFile, filename: str) -> str:
        """Save uploaded file to temporary location"""
        
        temp_dir = Path(self.storage_config["base_path"]) / "temporary"
        temp_file_path = temp_dir / filename
        
        async with aiofiles.open(temp_file_path, 'wb') as f:
            content = await file.read()
            await f.write(content)
            await file.seek(0)  # Reset file pointer
        
        return str(temp_file_path)
    
    async def _advanced_file_validation(self, file_path: str) -> Dict[str, Any]:
        """Advanced file validation using magic bytes and content analysis"""
        
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": [],
            "quarantine": False
        }
        
        try:
            # Magic byte validation
            detected_mime = magic.from_file(file_path, mime=True)
            file_extension = os.path.splitext(file_path)[1].lower()
            
            # Cross-check MIME type with extension
            expected_mime = mimetypes.guess_type(file_path)[0]
            if expected_mime and detected_mime != expected_mime:
                validation_result["warnings"].append(
                    f"MIME type mismatch: detected {detected_mime}, expected {expected_mime}"
                )
            
            # Check if detected MIME type is allowed
            if detected_mime not in self.validation_rules.allowed_mime_types:
                validation_result["valid"] = False
                validation_result["errors"].append(f"Detected MIME type {detected_mime} not allowed")
                validation_result["quarantine"] = True
            
            # File-specific validation
            if detected_mime == 'application/pdf':
                pdf_validation = await self._validate_pdf(file_path)
                if not pdf_validation["valid"]:
                    validation_result["errors"].extend(pdf_validation["errors"])
                    validation_result["quarantine"] = pdf_validation.get("quarantine", False)
            
            elif detected_mime.startswith('image/'):
                image_validation = await self._validate_image(file_path)
                if not image_validation["valid"]:
                    validation_result["errors"].extend(image_validation["errors"])
            
            elif detected_mime in ['application/vnd.ms-excel', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet']:
                excel_validation = await self._validate_excel(file_path)
                if not excel_validation["valid"]:
                    validation_result["warnings"].extend(excel_validation["warnings"])
            
        except Exception as e:
            logger.error(f"Advanced validation failed: {e}")
            validation_result["warnings"].append(f"Advanced validation error: {str(e)}")
        
        return validation_result
    
    async def _validate_pdf(self, file_path: str) -> Dict[str, Any]:
        """PDF-specific validation"""
        validation_result = {"valid": True, "errors": [], "quarantine": False}
        
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                # Check if PDF is encrypted
                if pdf_reader.is_encrypted:
                    validation_result["valid"] = False
                    validation_result["errors"].append("Encrypted PDFs not supported")
                    return validation_result
                
                # Check page count (reasonable limit)
                if len(pdf_reader.pages) > 500:
                    validation_result["valid"] = False
                    validation_result["errors"].append("PDF has too many pages (max 500)")
                
                # Check for suspicious content
                for i, page in enumerate(pdf_reader.pages[:5]):  # Check first 5 pages
                    text = page.extract_text().lower()
                    suspicious_keywords = ['javascript:', '<script', '<?php', 'eval(']
                    if any(keyword in text for keyword in suspicious_keywords):
                        validation_result["valid"] = False
                        validation_result["quarantine"] = True
                        validation_result["errors"].append("Suspicious content detected in PDF")
                        break
        
        except Exception as e:
            validation_result["valid"] = False
            validation_result["errors"].append(f"PDF validation error: {str(e)}")
        
        return validation_result
    
    async def _validate_image(self, file_path: str) -> Dict[str, Any]:
        """Image-specific validation"""
        validation_result = {"valid": True, "errors": []}
        
        try:
            with Image.open(file_path) as img:
                # Check image dimensions
                width, height = img.size
                max_dimension = 10000  # 10k pixels max
                
                if width > max_dimension or height > max_dimension:
                    validation_result["valid"] = False
                    validation_result["errors"].append(f"Image dimensions too large: {width}x{height}")
                
                # Check image mode
                if img.mode not in ['RGB', 'RGBA', 'L', 'P']:
                    validation_result["errors"].append(f"Unsupported image mode: {img.mode}")
                
                # Verify image integrity
                img.verify()
        
        except Exception as e:
            validation_result["valid"] = False
            validation_result["errors"].append(f"Image validation error: {str(e)}")
        
        return validation_result
    
    async def _validate_excel(self, file_path: str) -> Dict[str, Any]:
        """Excel-specific validation"""
        validation_result = {"valid": True, "warnings": []}
        
        try:
            # Read with pandas to check structure
            excel_file = pd.ExcelFile(file_path)
            
            # Check number of sheets
            if len(excel_file.sheet_names) > 20:
                validation_result["warnings"].append("Excel file has many sheets, processing may be slow")
            
            # Check first sheet for basic structure
            first_sheet = pd.read_excel(file_path, nrows=5)
            if first_sheet.empty:
                validation_result["warnings"].append("Excel file appears to be empty")
        
        except Exception as e:
            validation_result["warnings"].append(f"Excel validation warning: {str(e)}")
        
        return validation_result
    
    async def _virus_scan(self, file_path: str) -> Dict[str, Any]:
        """Virus scan simulation (integrate with actual antivirus)"""
        # This is a placeholder - integrate with actual antivirus solution
        # such as ClamAV, Windows Defender API, or cloud-based scanning
        
        scan_result = {
            "clean": True,
            "threats_found": [],
            "scan_time": 0.1
        }
        
        # Simulate scan based on file characteristics
        file_size = os.path.getsize(file_path)
        
        # Flag suspiciously large files
        if file_size > 100 * 1024 * 1024:  # 100MB
            scan_result["threats_found"].append("Suspiciously large file")
            scan_result["clean"] = False
        
        # Check file extension for high-risk types
        file_extension = os.path.splitext(file_path)[1].lower()
        high_risk_extensions = ['.exe', '.bat', '.cmd', '.scr', '.vbs', '.js']
        
        if file_extension in high_risk_extensions:
            scan_result["threats_found"].append("High-risk file extension")
            scan_result["clean"] = False
        
        return scan_result
    
    async def _extract_file_metadata(self, file_path: str, upload_file: UploadFile) -> Dict[str, Any]:
        """Extract comprehensive file metadata"""
        
        stat_result = os.stat(file_path)
        
        metadata = {
            "original_filename": upload_file.filename,
            "file_size": stat_result.st_size,
            "mime_type": magic.from_file(file_path, mime=True),
            "file_extension": os.path.splitext(upload_file.filename)[1].lower(),
            "created_date": datetime.fromtimestamp(stat_result.st_ctime),
            "modified_date": datetime.fromtimestamp(stat_result.st_mtime),
            "file_hash": await self._calculate_file_hash(file_path),
            "encoding": None,
            "page_count": None,
            "image_dimensions": None,
            "content_type": upload_file.content_type
        }
        
        # Type-specific metadata extraction
        mime_type = metadata["mime_type"]
        
        if mime_type == 'application/pdf':
            pdf_metadata = await self._extract_pdf_metadata(file_path)
            metadata.update(pdf_metadata)
        
        elif mime_type.startswith('image/'):
            image_metadata = await self._extract_image_metadata(file_path)
            metadata.update(image_metadata)
        
        elif mime_type in ['text/plain', 'text/csv']:
            text_metadata = await self._extract_text_metadata(file_path)
            metadata.update(text_metadata)
        
        return metadata
    
    async def _calculate_file_hash(self, file_path: str, algorithm: str = 'sha256') -> str:
        """Calculate file hash for integrity verification"""
        
        hash_func = hashlib.sha256()
        
        async with aiofiles.open(file_path, 'rb') as f:
            while chunk := await f.read(8192):
                hash_func.update(chunk)
        
        return hash_func.hexdigest()
    
    async def _extract_pdf_metadata(self, file_path: str) -> Dict[str, Any]:
        """Extract PDF-specific metadata"""
        
        metadata = {"page_count": None, "pdf_version": None, "creator": None, "producer": None}
        
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                metadata["page_count"] = len(pdf_reader.pages)
                
                if pdf_reader.metadata:
                    metadata["creator"] = pdf_reader.metadata.get('/Creator', '').strip()
                    metadata["producer"] = pdf_reader.metadata.get('/Producer', '').strip()
                    
                    # Clean sensitive metadata
                    if len(metadata["creator"]) > 100:
                        metadata["creator"] = metadata["creator"][:100] + "..."
                    if len(metadata["producer"]) > 100:
                        metadata["producer"] = metadata["producer"][:100] + "..."
        
        except Exception as e:
            logger.warning(f"Failed to extract PDF metadata: {e}")
        
        return metadata
    
    async def _extract_image_metadata(self, file_path: str) -> Dict[str, Any]:
        """Extract image-specific metadata"""
        
        metadata = {"image_dimensions": None, "color_mode": None, "dpi": None}
        
        try:
            with Image.open(file_path) as img:
                metadata["image_dimensions"] = f"{img.size[0]}x{img.size[1]}"
                metadata["color_mode"] = img.mode
                
                # Get DPI if available
                if hasattr(img, 'info') and 'dpi' in img.info:
                    metadata["dpi"] = img.info['dpi']
        
        except Exception as e:
            logger.warning(f"Failed to extract image metadata: {e}")
        
        return metadata
    
    async def _extract_text_metadata(self, file_path: str) -> Dict[str, Any]:
        """Extract text file metadata"""
        
        metadata = {"encoding": None, "line_count": None, "character_count": None}
        
        try:
            # Detect encoding
            with open(file_path, 'rb') as f:
                raw_data = f.read(10000)  # Read first 10KB
                import chardet
                encoding_result = chardet.detect(raw_data)
                metadata["encoding"] = encoding_result.get('encoding', 'unknown')
            
            # Count lines and characters
            with open(file_path, 'r', encoding=metadata["encoding"]) as f:
                content = f.read()
                metadata["line_count"] = content.count('\n') + 1
                metadata["character_count"] = len(content)
        
        except Exception as e:
            logger.warning(f"Failed to extract text metadata: {e}")
        
        return metadata
    
    async def _process_file_content(
        self, 
        file_path: str, 
        evidence_type: EvidenceType,
        file_metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Process file content for text extraction and analysis"""
        
        processing_result = {
            "extracted_text": None,
            "metadata": {},
            "warnings": []
        }
        
        mime_type = file_metadata.get("mime_type", "")
        
        try:
            if mime_type == 'application/pdf':
                processing_result = await self._process_pdf_content(file_path)
            
            elif mime_type.startswith('image/'):
                processing_result = await self._process_image_content(file_path)
            
            elif mime_type in ['text/plain', 'text/csv']:
                processing_result = await self._process_text_content(file_path, file_metadata)
            
            elif mime_type in ['application/vnd.ms-excel', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet']:
                processing_result = await self._process_excel_content(file_path)
            
            else:
                processing_result["warnings"].append(f"Content processing not available for {mime_type}")
        
        except Exception as e:
            processing_result["warnings"].append(f"Content processing failed: {str(e)}")
            logger.error(f"Content processing error: {e}")
        
        return processing_result
    
    async def _process_pdf_content(self, file_path: str) -> Dict[str, Any]:
        """Extract text content from PDF"""
        
        result = {"extracted_text": "", "metadata": {}, "warnings": []}
        
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                text_content = []
                page_count = 0
                
                for page in pdf_reader.pages:
                    page_text = page.extract_text()
                    if page_text.strip():
                        text_content.append(page_text)
                        page_count += 1
                    
                    # Limit processing for very large PDFs
                    if page_count >= 50:
                        result["warnings"].append("PDF processing limited to first 50 pages")
                        break
                
                result["extracted_text"] = "\n\n".join(text_content)
                result["metadata"]["processed_pages"] = page_count
                result["metadata"]["total_characters"] = len(result["extracted_text"])
        
        except Exception as e:
            result["warnings"].append(f"PDF text extraction failed: {str(e)}")
        
        return result
    
    async def _process_image_content(self, file_path: str) -> Dict[str, Any]:
        """Extract text from image using OCR"""
        
        result = {"extracted_text": "", "metadata": {}, "warnings": []}
        
        try:
            # Preprocess image for better OCR
            with Image.open(file_path) as img:
                # Convert to grayscale
                if img.mode != 'L':
                    img = img.convert('L')
                
                # Enhance contrast
                enhancer = ImageEnhance.Contrast(img)
                img = enhancer.enhance(2.0)
                
                # Resize if too small or too large
                width, height = img.size
                if width < 300 or height < 300:
                    scale_factor = max(300/width, 300/height)
                    new_size = (int(width * scale_factor), int(height * scale_factor))
                    img = img.resize(new_size, Image.Resampling.LANCZOS)
                elif width > 3000 or height > 3000:
                    scale_factor = min(3000/width, 3000/height)
                    new_size = (int(width * scale_factor), int(height * scale_factor))
                    img = img.resize(new_size, Image.Resampling.LANCZOS)
                
                # Perform OCR
                extracted_text = pytesseract.image_to_string(img, config='--psm 6')
                
                result["extracted_text"] = extracted_text.strip()
                result["metadata"]["ocr_confidence"] = "estimated"  # Could use pytesseract confidence if available
                result["metadata"]["processed_image_size"] = f"{img.size[0]}x{img.size[1]}"
        
        except Exception as e:
            result["warnings"].append(f"Image OCR failed: {str(e)}")
        
        return result
    
    async def _process_text_content(self, file_path: str, file_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Process plain text and CSV files"""
        
        result = {"extracted_text": "", "metadata": {}, "warnings": []}
        
        try:
            encoding = file_metadata.get("encoding", "utf-8")
            
            with open(file_path, 'r', encoding=encoding) as f:
                content = f.read()
                
                # Limit content length
                max_length = 100000  # 100KB of text
                if len(content) > max_length:
                    result["extracted_text"] = content[:max_length] + "\n... [Content truncated]"
                    result["warnings"].append("Text content truncated due to size")
                else:
                    result["extracted_text"] = content
                
                result["metadata"]["character_count"] = len(content)
                result["metadata"]["line_count"] = content.count('\n') + 1
        
        except Exception as e:
            result["warnings"].append(f"Text processing failed: {str(e)}")
        
        return result
    
    async def _process_excel_content(self, file_path: str) -> Dict[str, Any]:
        """Process Excel files and extract summary information"""
        
        result = {"extracted_text": "", "metadata": {}, "warnings": []}
        
        try:
            excel_file = pd.ExcelFile(file_path)
            
            summary_info = []
            sheet_summaries = []
            
            for sheet_name in excel_file.sheet_names[:5]:  # Limit to first 5 sheets
                try:
                    df = pd.read_excel(file_path, sheet_name=sheet_name, nrows=100)
                    
                    sheet_summary = {
                        "sheet_name": sheet_name,
                        "rows": len(df),
                        "columns": len(df.columns),
                        "column_names": df.columns.tolist()[:10]  # First 10 column names
                    }
                    
                    sheet_summaries.append(sheet_summary)
                    
                    # Create text summary
                    summary_info.append(f"Sheet: {sheet_name}")
                    summary_info.append(f"Dimensions: {len(df)} rows Ã— {len(df.columns)} columns")
                    summary_info.append(f"Columns: {', '.join(df.columns.tolist()[:5])}")
                    if len(df.columns) > 5:
                        summary_info.append("... and more columns")
                    summary_info.append("")
                
                except Exception as e:
                    result["warnings"].append(f"Failed to process sheet {sheet_name}: {str(e)}")
            
            result["extracted_text"] = "\n".join(summary_info)
            result["metadata"]["sheet_count"] = len(excel_file.sheet_names)
            result["metadata"]["processed_sheets"] = len(sheet_summaries)
            result["metadata"]["sheet_summaries"] = sheet_summaries
        
        except Exception as e:
            result["warnings"].append(f"Excel processing failed: {str(e)}")
        
        return result
    
    def _generate_unique_filename(self, original_filename: str, evidence_id: str) -> str:
        """Generate unique filename while preserving extension"""
        
        # Clean original filename
        base_name = os.path.splitext(original_filename)[0]
        extension = os.path.splitext(original_filename)[1].lower()
        
        # Remove problematic characters
        base_name = "".join(c for c in base_name if c.isalnum() or c in (' ', '-', '_')).strip()
        base_name = base_name[:50]  # Limit length
        
        # Create unique filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_filename = f"{evidence_id}_{timestamp}_{base_name}{extension}"
        
        return unique_filename
    
    async def _store_file(
        self, 
        temp_file_path: str, 
        assessment_id: str, 
        evidence_id: str, 
        filename: str
    ) -> str:
        """Store file in final location (local or cloud)"""
        
        # Create assessment-specific directory
        storage_path = Path(self.storage_config["base_path"]) / "processed" / assessment_id
        storage_path.mkdir(parents=True, exist_ok=True)
        
        final_file_path = storage_path / filename
        
        # Copy file to final location
        shutil.copy2(temp_file_path, final_file_path)
        
        # Store in cloud if enabled
        if self.storage_config.get("use_cloud_storage") and self.s3_client:
            try:
                cloud_key = f"assessments/{assessment_id}/evidence/{filename}"
                
                with open(final_file_path, 'rb') as f:
                    self.s3_client.upload_fileobj(
                        f,
                        self.storage_config["aws_bucket"],
                        cloud_key,
                        ExtraArgs={
                            'ServerSideEncryption': 'AES256' if self.storage_config.get("enable_encryption") else None
                        }
                    )
                
                logger.info(f"File uploaded to cloud storage: {cloud_key}")
            
            except Exception as e:
                logger.error(f"Failed to upload to cloud storage: {e}")
                # Continue with local storage
        
        return str(final_file_path)
    
    async def _generate_thumbnail(self, image_path: str, evidence_id: str) -> Optional[str]:
        """Generate thumbnail for image files"""
        
        try:
            thumbnail_dir = Path(self.storage_config["base_path"]) / "thumbnails"
            thumbnail_path = thumbnail_dir / f"{evidence_id}_thumb.jpg"
            
            with Image.open(image_path) as img:
                # Create thumbnail
                img.thumbnail((300, 300), Image.Resampling.LANCZOS)
                
                # Convert to RGB if necessary
                if img.mode in ('RGBA', 'LA', 'P'):
                    background = Image.new('RGB', img.size, (255, 255, 255))
                    background.paste(img, mask=img.split()[-1] if img.mode in ('RGBA', 'LA') else None)
                    img = background
                
                # Save thumbnail
                img.save(thumbnail_path, 'JPEG', quality=85)
            
            return str(thumbnail_path)
        
        except Exception as e:
            logger.error(f"Failed to generate thumbnail: {e}")
            return None
    
    async def _quarantine_file(self, file_path: str, evidence_id: str, reason: str):
        """Move file to quarantine directory"""
        
        quarantine_dir = Path(self.storage_config["base_path"]) / "quarantine"
        quarantine_path = quarantine_dir / f"{evidence_id}_quarantined"
        
        # Move file to quarantine
        shutil.move(file_path, quarantine_path)
        
        # Create quarantine record
        quarantine_info = {
            "evidence_id": evidence_id,
            "quarantine_date": datetime.utcnow().isoformat(),
            "reason": reason,
            "original_path": file_path,
            "quarantine_path": str(quarantine_path)
        }
        
        quarantine_log_path = quarantine_dir / "quarantine_log.json"
        
        # Append to quarantine log
        quarantine_log = []
        if quarantine_log_path.exists():
            with open(quarantine_log_path, 'r') as f:
                quarantine_log = json.load(f)
        
        quarantine_log.append(quarantine_info)
        
        with open(quarantine_log_path, 'w') as f:
            json.dump(quarantine_log, f, indent=2)
        
        logger.warning(f"File quarantined: {evidence_id} - {reason}")
    
    async def _create_evidence_record(
        self,
        evidence_id: str,
        assessment_id: str,
        response_id: Optional[str],
        file_metadata: Dict[str, Any],
        file_path: str,
        evidence_type: EvidenceType,
        title: str,
        description: Optional[str],
        uploaded_by: str,
        extracted_text: Optional[str],
        processing_metadata: Dict[str, Any],
        db_session
    ) -> AssessmentEvidence:
        """Create database record for evidence"""
        
        if not db_session:
            raise ValueError("Database session required")
        
        evidence_record = AssessmentEvidence(
            id=uuid.UUID(evidence_id),
            assessment_id=uuid.UUID(assessment_id),
            response_id=uuid.UUID(response_id) if response_id else None,
            file_name=os.path.basename(file_path),
            original_filename=file_metadata["original_filename"],
            file_path=file_path,
            file_size=file_metadata["file_size"],
            file_type=file_metadata["mime_type"],
            file_hash=file_metadata["file_hash"],
            evidence_type=evidence_type,
            title=title,
            description=description,
            uploaded_by=uuid.UUID(uploaded_by),
            extracted_text=extracted_text,
            document_summary=None,  # Could be populated with AI summarization
            processing_status=ProcessingStatus.COMPLETED.value,
            is_processed=True,
            is_validated=False  # Requires manual validation
        )
        
        db_session.add(evidence_record)
        db_session.commit()
        db_session.refresh(evidence_record)
        
        return evidence_record

# ================================================================================
# EVIDENCE RETRIEVAL AND MANAGEMENT FUNCTIONS
# ================================================================================

class EvidenceRetrievalService:
    """Service for retrieving and managing existing evidence"""
    
    def __init__(self, storage_config: Dict[str, Any] = None):
        self.storage_config = storage_config or {"base_path": "uploads/evidence"}
    
    async def get_evidence_file(self, evidence_id: str, db_session) -> Tuple[Optional[str], Dict[str, Any]]:
        """Retrieve evidence file path and metadata"""
        
        evidence = db_session.query(AssessmentEvidence).filter(
            AssessmentEvidence.id == uuid.UUID(evidence_id)
        ).first()
        
        if not evidence:
            return None, {"error": "Evidence not found"}
        
        if not os.path.exists(evidence.file_path):
            return None, {"error": "Evidence file not found on storage"}
        
        metadata = {
            "evidence_id": str(evidence.id),
            "original_filename": evidence.original_filename,
            "file_size": evidence.file_size,
            "file_type": evidence.file_type,
            "evidence_type": evidence.evidence_type.value,
            "upload_date": evidence.created_at.isoformat(),
            "processing_status": evidence.processing_status
        }
        
        return evidence.file_path, metadata
    
    async def delete_evidence(self, evidence_id: str, db_session) -> bool:
        """Soft delete evidence (move to archive)"""
        
        evidence = db_session.query(AssessmentEvidence).filter(
            AssessmentEvidence.id == uuid.UUID(evidence_id)
        ).first()
        
        if not evidence:
            return False
        
        # Move file to archive
        archive_dir = Path(self.storage_config["base_path"]) / "archived"
        archive_dir.mkdir(exist_ok=True)
        
        if os.path.exists(evidence.file_path):
            archive_path = archive_dir / f"{evidence_id}_{os.path.basename(evidence.file_path)}"
            shutil.move(evidence.file_path, archive_path)
            evidence.file_path = str(archive_path)
        
        # Soft delete in database
        evidence.soft_delete()
        db_session.commit()
        
        return True

# ================================================================================
# FACTORY FUNCTIONS
# ================================================================================

def create_evidence_management_service(storage_config: Dict[str, Any] = None) -> EvidenceManagementService:
    """Factory function to create evidence management service"""
    return EvidenceManagementService(storage_config)

def create_evidence_retrieval_service(storage_config: Dict[str, Any] = None) -> EvidenceRetrievalService:
    """Factory function to create evidence retrieval service"""
    return EvidenceRetrievalService(storage_config)

print("âœ… Evidence Management Service Loaded Successfully!")
print("Features:")
print("  ğŸ“ Comprehensive File Validation and Processing")
print("  ğŸ” Advanced Content Analysis with OCR")
print("  ğŸ¦  Virus Scanning and Security Validation")
print("  â˜ï¸ Cloud Storage Integration (AWS S3)")
print("  ğŸ–¼ï¸ Thumbnail Generation for Images")
print("  ğŸ“„ PDF and Excel Content Extraction")
print("  ğŸ” File Encryption and Integrity Verification")
print("  ğŸ“Š Detailed Processing Metadata")
print("  âš¡ Asynchronous Processing Pipeline")