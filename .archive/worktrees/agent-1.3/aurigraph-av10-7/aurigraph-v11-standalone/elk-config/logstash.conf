# Logstash Pipeline Configuration for Aurigraph V11
# Processes logs from multiple sources and sends to Elasticsearch

input {
  # TCP input for direct application logs
  tcp {
    port => 5000
    codec => json_lines
    type => "application"
    tags => ["aurigraph", "tcp"]
  }

  # UDP input for high-throughput logs
  udp {
    port => 5000
    codec => json_lines
    type => "application"
    tags => ["aurigraph", "udp"]
  }

  # File input for application logs
  file {
    path => "/logs/application.log"
    start_position => "beginning"
    sincedb_path => "/usr/share/logstash/data/sincedb_application"
    codec => json
    type => "application"
    tags => ["aurigraph", "file"]
  }

  # Nginx access logs
  file {
    path => "/var/log/nginx/access.log"
    start_position => "beginning"
    sincedb_path => "/usr/share/logstash/data/sincedb_nginx"
    type => "nginx"
    tags => ["nginx", "access"]
  }

  # Nginx error logs
  file {
    path => "/var/log/nginx/error.log"
    start_position => "beginning"
    sincedb_path => "/usr/share/logstash/data/sincedb_nginx_error"
    type => "nginx"
    tags => ["nginx", "error"]
  }

  # Filebeat input
  beats {
    port => 5044
    type => "filebeat"
    tags => ["filebeat"]
  }
}

filter {
  # Application log processing
  if [type] == "application" {
    # Parse JSON if not already parsed
    if [message] =~ /^\{/ {
      json {
        source => "message"
        target => "parsed"
      }

      # Promote parsed fields to root
      mutate {
        rename => {
          "[parsed][timestamp]" => "log_timestamp"
          "[parsed][level]" => "log_level"
          "[parsed][logger]" => "logger_name"
          "[parsed][message]" => "log_message"
          "[parsed][category]" => "log_category"
          "[parsed][correlationId]" => "correlation_id"
          "[parsed][txId]" => "transaction_id"
          "[parsed][userId]" => "user_id"
          "[parsed][sessionId]" => "session_id"
          "[parsed][operation]" => "operation"
          "[parsed][durationMs]" => "duration_ms"
          "[parsed][tps]" => "transactions_per_second"
          "[parsed][statusCode]" => "http_status"
          "[parsed][method]" => "http_method"
          "[parsed][path]" => "http_path"
        }
      }
    }

    # Add service metadata
    mutate {
      add_field => {
        "service_name" => "aurigraph-v11"
        "service_version" => "11.3.1"
        "service_platform" => "java-quarkus"
      }
    }

    # Convert numeric fields
    mutate {
      convert => {
        "duration_ms" => "float"
        "transactions_per_second" => "float"
        "http_status" => "integer"
      }
    }

    # Classify log severity
    if [log_level] == "ERROR" or [log_level] == "FATAL" {
      mutate {
        add_field => { "severity" => "critical" }
      }
    } else if [log_level] == "WARN" {
      mutate {
        add_field => { "severity" => "warning" }
      }
    } else {
      mutate {
        add_field => { "severity" => "normal" }
      }
    }
  }

  # Nginx access log processing
  if [type] == "nginx" and "access" in [tags] {
    grok {
      match => {
        "message" => '%{IPORHOST:remote_addr} - %{USER:remote_user} \[%{HTTPDATE:time_local}\] "%{WORD:request_method} %{URIPATHPARAM:request_uri} HTTP/%{NUMBER:http_version}" %{NUMBER:status} %{NUMBER:body_bytes_sent} "%{DATA:http_referer}" "%{DATA:http_user_agent}"'
      }
    }

    date {
      match => ["time_local", "dd/MMM/yyyy:HH:mm:ss Z"]
      target => "@timestamp"
    }

    mutate {
      convert => {
        "status" => "integer"
        "body_bytes_sent" => "integer"
      }
      add_field => {
        "service_name" => "nginx"
        "log_category" => "HTTP"
      }
    }
  }

  # Nginx error log processing
  if [type] == "nginx" and "error" in [tags] {
    grok {
      match => {
        "message" => '%{DATESTAMP:error_time} \[%{LOGLEVEL:log_level}\] %{GREEDYDATA:error_message}'
      }
    }

    mutate {
      add_field => {
        "service_name" => "nginx"
        "log_category" => "ERROR"
        "severity" => "warning"
      }
    }
  }

  # Add geolocation for IP addresses
  if [remote_addr] and [remote_addr] !~ /^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.|127\.)/ {
    geoip {
      source => "remote_addr"
      target => "geoip"
    }
  }

  # Add timestamp if missing
  if ![log_timestamp] and ![@timestamp] {
    mutate {
      add_field => { "log_timestamp" => "%{@timestamp}" }
    }
  }

  # Remove unnecessary fields
  mutate {
    remove_field => ["parsed", "message"]
  }
}

output {
  # Send to Elasticsearch with daily indices
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "aurigraph-logs-%{[service_name]}-%{+YYYY.MM.dd}"
    template_name => "aurigraph-logs"
    template_overwrite => true
  }

  # Debug output (comment out in production)
  # stdout {
  #   codec => rubydebug
  # }

  # Conditional output for errors (send to separate index)
  if [severity] == "critical" or [log_level] == "ERROR" {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "aurigraph-errors-%{+YYYY.MM.dd}"
    }
  }

  # Performance metrics to separate index
  if [log_category] == "PERFORMANCE" {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "aurigraph-performance-%{+YYYY.MM.dd}"
    }
  }

  # Security events to separate index
  if [log_category] == "SECURITY" {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "aurigraph-security-%{+YYYY.MM.dd}"
    }
  }
}
