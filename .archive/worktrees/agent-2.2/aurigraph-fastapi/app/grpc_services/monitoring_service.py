"""
Monitoring Service gRPC implementation
Handles performance metrics and health monitoring with Protocol Buffers
"""

import asyncio
import logging
import psutil
import time
from typing import AsyncIterator
import grpc
from datetime import datetime

from generated.aurigraph_pb2 import (
    PerformanceMetrics, MetricsStreamRequest, HealthStatus, Empty
)
from generated.aurigraph_pb2_grpc import MonitoringServiceServicer

logger = logging.getLogger(__name__)


class MonitoringServiceImpl(MonitoringServiceServicer):
    """
    Monitoring service implementation
    Provides real-time performance metrics and health status
    """
    
    def __init__(self, monitoring_service=None):
        self.monitoring_service = monitoring_service
        self.start_time = time.time()
        self.metrics_history = []
        self.current_tps = 0.0
        self.total_transactions = 0
        self.total_blocks = 0
        self.last_metrics_update = time.time()
        
        # Performance counters
        self.transaction_counter = 0
        self.block_counter = 0
        self.finality_times = []
        
    async def GetMetrics(
        self,
        request: Empty,
        context: grpc.aio.ServicerContext
    ) -> PerformanceMetrics:
        """
        Get current performance metrics
        """
        try:
            if self.monitoring_service:
                # Use actual monitoring service
                metrics_data = await self.monitoring_service.get_current_metrics()
                return PerformanceMetrics(**metrics_data)
            else:
                # Generate mock metrics for development
                metrics = self._generate_current_metrics()
                self.metrics_history.append(metrics)
                
                # Keep only last 1000 metrics
                if len(self.metrics_history) > 1000:
                    self.metrics_history = self.metrics_history[-1000:]
                
                return metrics
                
        except Exception as e:
            logger.error(f"Error getting metrics: {e}")
            return self._get_error_metrics()
    
    async def StreamMetrics(
        self,
        request: MetricsStreamRequest,
        context: grpc.aio.ServicerContext
    ) -> AsyncIterator[PerformanceMetrics]:
        """
        Stream performance metrics at specified intervals
        """
        try:
            interval = max(1, request.interval_seconds)  # Minimum 1 second
            logger.info(f"Starting metrics stream with {interval}s interval")
            
            while not context.cancelled():
                try:
                    # Get current metrics
                    if self.monitoring_service:
                        metrics_data = await self.monitoring_service.get_current_metrics()
                        metrics = PerformanceMetrics(**metrics_data)
                    else:
                        metrics = self._generate_current_metrics()
                    
                    # Filter metrics if specified
                    if request.metrics:
                        metrics = self._filter_metrics(metrics, request.metrics)
                    
                    yield metrics
                    
                    # Wait for next interval
                    await asyncio.sleep(interval)
                    
                except grpc.RpcError:
                    # Client disconnected
                    break
                except Exception as e:
                    logger.error(f"Error in metrics stream: {e}")
                    yield self._get_error_metrics()
                    await asyncio.sleep(interval)
                    
            logger.info("Metrics stream ended")
            
        except Exception as e:
            logger.error(f"Error starting metrics stream: {e}")
            await context.abort(grpc.StatusCode.INTERNAL, f"Stream error: {str(e)}")
    
    async def GetHealthStatus(
        self,
        request: Empty,
        context: grpc.aio.ServicerContext
    ) -> HealthStatus:
        """
        Get current health status of all services
        """
        try:
            if self.monitoring_service:
                health_data = await self.monitoring_service.get_health_status()
                return HealthStatus(**health_data)
            else:
                # Generate mock health status
                services = {
                    "consensus": self._check_service_health("consensus"),
                    "transaction_processor": self._check_service_health("transaction_processor"),
                    "quantum_crypto": self._check_service_health("quantum_crypto"),
                    "p2p_network": self._check_service_health("p2p_network"),
                    "database": self._check_service_health("database"),
                    "grpc_server": True,  # This service is running if we're here
                }
                
                overall_healthy = all(services.values())
                uptime = int(time.time() - self.start_time)
                
                return HealthStatus(
                    healthy=overall_healthy,
                    services=services,
                    uptime_seconds=uptime,
                    version="11.0.0"
                )
                
        except Exception as e:
            logger.error(f"Error getting health status: {e}")
            return HealthStatus(
                healthy=False,
                services={"error": False},
                uptime_seconds=0,
                version="11.0.0"
            )
    
    def _generate_current_metrics(self) -> PerformanceMetrics:
        """
        Generate current performance metrics
        """
        try:
            current_time = time.time()
            time_diff = current_time - self.last_metrics_update
            
            # Simulate TPS calculation
            if time_diff > 0:
                # Simulate varying TPS (500K - 1M range for demo)
                import random
                base_tps = 776000  # Current achieved TPS
                variation = random.uniform(0.8, 1.2)  # Â±20% variation
                self.current_tps = base_tps * variation
                
                # Update transaction counter
                new_transactions = int(self.current_tps * time_diff)
                self.transaction_counter += new_transactions
                self.total_transactions += new_transactions
                
                # Update block counter (assuming 10,000 tx per block)
                new_blocks = new_transactions // 10000
                if new_blocks > 0:
                    self.block_counter += new_blocks
                    self.total_blocks += new_blocks
            
            # Get system metrics
            cpu_usage = psutil.cpu_percent(interval=None)
            memory = psutil.virtual_memory()
            memory_usage = memory.percent
            
            # Calculate finality time (mock)
            import random
            finality_time = random.uniform(80, 120)  # 80-120ms range
            self.finality_times.append(finality_time)
            if len(self.finality_times) > 100:
                self.finality_times = self.finality_times[-100:]
            
            avg_finality = sum(self.finality_times) / len(self.finality_times)
            
            # Network latency simulation
            network_latency = random.uniform(10, 50)  # 10-50ms
            
            self.last_metrics_update = current_time
            
            return PerformanceMetrics(
                tps=self.current_tps,
                finality_time_ms=avg_finality,
                total_transactions=self.total_transactions,
                total_blocks=self.total_blocks,
                network_latency_ms=network_latency,
                cpu_usage=cpu_usage,
                memory_usage=memory_usage,
                timestamp=int(current_time * 1000)
            )
            
        except Exception as e:
            logger.error(f"Error generating metrics: {e}")
            return self._get_error_metrics()
    
    def _get_error_metrics(self) -> PerformanceMetrics:
        """
        Get error metrics when normal metrics fail
        """
        return PerformanceMetrics(
            tps=0.0,
            finality_time_ms=0.0,
            total_transactions=0,
            total_blocks=0,
            network_latency_ms=0.0,
            cpu_usage=0.0,
            memory_usage=0.0,
            timestamp=int(time.time() * 1000)
        )
    
    def _filter_metrics(self, metrics: PerformanceMetrics, requested_metrics: list) -> PerformanceMetrics:
        """
        Filter metrics to only include requested fields
        """
        try:
            # Create filtered metrics object
            filtered = PerformanceMetrics()
            
            metric_fields = {
                'tps': metrics.tps,
                'finality_time_ms': metrics.finality_time_ms,
                'total_transactions': metrics.total_transactions,
                'total_blocks': metrics.total_blocks,
                'network_latency_ms': metrics.network_latency_ms,
                'cpu_usage': metrics.cpu_usage,
                'memory_usage': metrics.memory_usage,
                'timestamp': metrics.timestamp
            }
            
            for field in requested_metrics:
                if field in metric_fields:
                    setattr(filtered, field, metric_fields[field])
            
            return filtered
            
        except Exception as e:
            logger.error(f"Error filtering metrics: {e}")
            return metrics  # Return original on error
    
    def _check_service_health(self, service_name: str) -> bool:
        """
        Check health status of a specific service
        """
        try:
            # Mock service health checks
            # In real implementation, this would check actual service status
            
            if service_name == "consensus":
                # Check if consensus is working
                return True  # Assume healthy for demo
            elif service_name == "transaction_processor":
                # Check transaction processing
                return True
            elif service_name == "quantum_crypto":
                # Check quantum crypto service
                return True
            elif service_name == "p2p_network":
                # Check P2P network connectivity
                return True
            elif service_name == "database":
                # Check database connectivity
                return True
            else:
                return True
                
        except Exception as e:
            logger.error(f"Error checking {service_name} health: {e}")
            return False
    
    def update_transaction_count(self, count: int):
        """
        Update transaction count from external source
        """
        self.transaction_counter += count
        self.total_transactions += count
    
    def update_block_count(self, count: int):
        """
        Update block count from external source
        """
        self.block_counter += count
        self.total_blocks += count
    
    def add_finality_time(self, finality_ms: float):
        """
        Add a finality time measurement
        """
        self.finality_times.append(finality_ms)
        if len(self.finality_times) > 1000:
            self.finality_times = self.finality_times[-1000:]
    
    def get_stats(self) -> dict:
        """
        Get monitoring service statistics
        """
        return {
            "uptime_seconds": int(time.time() - self.start_time),
            "metrics_collected": len(self.metrics_history),
            "current_tps": self.current_tps,
            "total_transactions": self.total_transactions,
            "total_blocks": self.total_blocks,
            "avg_finality_ms": (
                sum(self.finality_times) / len(self.finality_times)
                if self.finality_times else 0.0
            )
        }